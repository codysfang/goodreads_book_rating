{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(409782)\n",
    "label = \"rating_label\"\n",
    "random_state_const = 10987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f\"train_final.csv\")\n",
    "test = pd.read_csv(f\"test_final.csv\")\n",
    "\n",
    "train_y = train.pop(label)\n",
    "train_X = train\n",
    "test_y = test.pop(label)\n",
    "test_X = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def report(a, b):\n",
    "    reports = [\n",
    "                metrics.accuracy_score(a, b), \n",
    "                metrics.precision_score(a, b, average=\"macro\"),\n",
    "                metrics.recall_score(a, b, average=\"macro\"),\n",
    "                metrics.f1_score(a, b, average=\"macro\")\n",
    "               ]\n",
    "    return reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_accuracy</th>\n",
       "      <th>split1_test_accuracy</th>\n",
       "      <th>split2_test_accuracy</th>\n",
       "      <th>split3_test_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th>split0_test_f1_macro</th>\n",
       "      <th>split1_test_f1_macro</th>\n",
       "      <th>split2_test_f1_macro</th>\n",
       "      <th>split3_test_f1_macro</th>\n",
       "      <th>split4_test_f1_macro</th>\n",
       "      <th>mean_test_f1_macro</th>\n",
       "      <th>std_test_f1_macro</th>\n",
       "      <th>rank_test_f1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38.272890</td>\n",
       "      <td>8.861873</td>\n",
       "      <td>0.004383</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "      <td>0.705691</td>\n",
       "      <td>0.704336</td>\n",
       "      <td>0.701626</td>\n",
       "      <td>0.700542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327137</td>\n",
       "      <td>0.346354</td>\n",
       "      <td>0.317650</td>\n",
       "      <td>0.301085</td>\n",
       "      <td>0.336383</td>\n",
       "      <td>0.325722</td>\n",
       "      <td>0.015579</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.419557</td>\n",
       "      <td>8.650992</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>none</td>\n",
       "      <td>{'penalty': 'none'}</td>\n",
       "      <td>0.705420</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.702981</td>\n",
       "      <td>0.700813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>2</td>\n",
       "      <td>0.327674</td>\n",
       "      <td>0.345980</td>\n",
       "      <td>0.326158</td>\n",
       "      <td>0.304676</td>\n",
       "      <td>0.342058</td>\n",
       "      <td>0.329309</td>\n",
       "      <td>0.014559</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_penalty  \\\n",
       "0      38.272890      8.861873         0.004383        0.001646            l2   \n",
       "1      39.419557      8.650992         0.003099        0.001433          none   \n",
       "\n",
       "                params  split0_test_accuracy  split1_test_accuracy  \\\n",
       "0    {'penalty': 'l2'}              0.705691              0.704336   \n",
       "1  {'penalty': 'none'}              0.705420              0.699187   \n",
       "\n",
       "   split2_test_accuracy  split3_test_accuracy  ...  std_test_accuracy  \\\n",
       "0              0.701626              0.700542  ...           0.001898   \n",
       "1              0.702981              0.700813  ...           0.002103   \n",
       "\n",
       "   rank_test_accuracy  split0_test_f1_macro  split1_test_f1_macro  \\\n",
       "0                   1              0.327137              0.346354   \n",
       "1                   2              0.327674              0.345980   \n",
       "\n",
       "   split2_test_f1_macro  split3_test_f1_macro  split4_test_f1_macro  \\\n",
       "0              0.317650              0.301085              0.336383   \n",
       "1              0.326158              0.304676              0.342058   \n",
       "\n",
       "   mean_test_f1_macro  std_test_f1_macro  rank_test_f1_macro  \n",
       "0            0.325722           0.015579                   2  \n",
       "1            0.329309           0.014559                   1  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_classifier = LogisticRegression(random_state=random_state_const, max_iter=10000)\n",
    "possible_hyperparams = {'penalty': ['l2', 'none']}\n",
    "\n",
    "grid_search = GridSearchCV(logistic_classifier, possible_hyperparams, scoring=['accuracy','f1_macro'], refit=False)\n",
    "grid_search.fit(train_X, train_y)\n",
    "\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logis = LogisticRegression(random_state=10987, max_iter=10000, penalty='none')\n",
    "logis.fit(train_X, train_y)\n",
    "res = logis.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7049642315196185,\n",
       " 0.3529648401755386,\n",
       " 0.5819706074769507,\n",
       " 0.3214513234996019]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(pred_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_X = pd.read_csv(\"predict_final.csv\")\n",
    "predictions = pd.Series(logis.predict(predict_X))\n",
    "csv_file = pd.DataFrame(predictions, columns=[label])\n",
    "csv_file.insert(0, \"id\", predictions.index + 1)\n",
    "csv_file.to_csv(\"logisitic_output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
